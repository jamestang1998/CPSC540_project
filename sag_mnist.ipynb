{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bn8q67W5EllC",
    "outputId": "c08b100b-bc30-47e0-d372-b8f590400de7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My mom where\n",
      "I want my mom\n",
      "Trainloader size: 60000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable, grad\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import basic_sag, basic_saga, basic_sgd\n",
    "\n",
    "torch.manual_seed(0)\n",
    "batch_size = 1\n",
    "n_epoch = 2\n",
    "\n",
    "\n",
    "class CustomRandomSampler:\n",
    "    \"\"\"Samples elements randomly, without replacement.\n",
    "\n",
    "    Arguments:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "    \"\"\"\n",
    "    def __init__(self, data_source, iter_list):\n",
    "        self.data_source = data_source\n",
    "        self.iter_list = torch.LongTensor(iter_list)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.iter_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.iter_list)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 5)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        arg : neural net, data\n",
    "        goal : predict x's classification\n",
    "        return : classification\n",
    "        '''\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "\t'''print image'''\n",
    "\timg = img / 2 + 0.5     # unnormalize\n",
    "\tnpimg = img.numpy()\n",
    "\tplt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "def partial_grad(model, loss_function, data, target, i_data, iter_list):\n",
    "    \"\"\"\n",
    "    arg : neural network, loss function, data,\n",
    "                target, full_grad (bool), iteration nÂ°i_data, list of initial data randomly picked\n",
    "    goal : apply partial grad and calculate loss between prediction and result\n",
    "    return : number of the data, loss\n",
    "    \"\"\"\n",
    "    outputs = model.forward(data)\n",
    "    loss = loss_function(outputs, target)\n",
    "    loss.backward()  # store gradient\n",
    "    return iter_list[i_data], loss\n",
    "\n",
    "def calculate_loss_grad(model, dataloader, loss_function, iter_list, n_samples):\n",
    "    \"\"\"\n",
    "    inputs : neural network, dataset, loss function, number of samples\n",
    "    goal : calculate the gradient and the loss\n",
    "    return : loss and gradient values\n",
    "    \"\"\"\n",
    "    full_loss_epoch = 0\n",
    "    grad_norm_epoch = 0\n",
    "    model.zero_grad()\n",
    "    for i_grad, data_grad in enumerate(dataloader):\n",
    "        inputs, labels = data_grad\n",
    "        # inputs, labels = Variable(inputs), Variable(labels)\n",
    "        # i_data_grad, loss_grad = partial_grad(model, loss_function, inputs, labels, i_grad, iter_list)\n",
    "        # full_loss_epoch += (1. / n_samples) * loss_grad.data[0]\n",
    "\n",
    "\n",
    "    return full_loss_epoch\n",
    "\n",
    "\n",
    "def populate_gradient(model, x, y, index, optimizer, criterion):\n",
    "    model.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.populate_initial_gradients(index)\n",
    "\n",
    "print('My mom where')\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='.', train=True,\n",
    "\t\t\t\t\t\t\t\t\t\tdownload=True, transform=transform)\n",
    "# n_samples = trainset.data.shape[0]\n",
    "n_samples = len(trainset)\n",
    "random_iter = np.random.randint(0, n_samples, n_samples * n_epoch)\n",
    "sampler = CustomRandomSampler(trainset, random_iter)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=sampler, shuffle=False)\n",
    "\n",
    "# gradloader = torch.utils.data.DataLoader(trainset, batch_size=1,\n",
    "# \t\t\t\t\t\t\t\t\t\t\tshuffle=False, num_workers=2) #to get the gradient for each epoch\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "# \t\t\t\t\t\t\t\t\t\tdownload=True, transform=transform)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "# \t\t\t\t\t\t\t\t\t\tshuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "\t\t\t'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# get some random training images\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "\n",
    "print('I want my mom')\n",
    "\n",
    "\n",
    "#Define constants\n",
    "learning_rate = 0.01\n",
    "\n",
    "#Define nets and parameters\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epoch = 0\n",
    "running_loss = 0.0\n",
    "grad_norm_epoch = [0 for i in range(n_epoch)]\n",
    "full_loss_epoch = [0 for i in range(n_epoch)]\n",
    "\n",
    "optm = basic_sag.SAG(net.parameters(), N=len(trainset), lr=learning_rate)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('Trainloader size:', len(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "i7s_jqsGFDBY"
   },
   "outputs": [],
   "source": [
    "# populating initial_grads\n",
    "for i, data in enumerate(trainset):\n",
    "    inputs, labels = data\n",
    "    inputs, labels = Variable(inputs.unsqueeze(0)), Variable(torch.tensor(labels).unsqueeze(0))\n",
    "    populate_gradient(net, inputs, labels, i, optm, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLRFLo3lHX-i",
    "outputId": "27b0867e-4def-409e-fba3-bad71c069b11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,  2500] loss: 2.307\n",
      "[0,  5000] loss: 2.304\n",
      "[0,  7500] loss: 2.306\n",
      "[0, 10000] loss: 2.306\n",
      "[0, 12500] loss: 2.307\n",
      "[0, 15000] loss: 2.307\n",
      "[0, 17500] loss: 2.307\n",
      "[0, 20000] loss: 2.305\n",
      "[0, 22500] loss: 2.306\n",
      "[0, 25000] loss: 2.306\n",
      "[0, 27500] loss: 2.307\n",
      "[0, 30000] loss: 2.308\n",
      "[0, 32500] loss: 2.307\n",
      "[0, 35000] loss: 2.304\n",
      "[0, 37500] loss: 2.304\n",
      "[0, 40000] loss: 2.303\n",
      "[0, 42500] loss: 2.307\n",
      "[0, 45000] loss: 2.305\n",
      "[0, 47500] loss: 2.301\n",
      "[0, 50000] loss: 2.292\n",
      "[0, 52500] loss: 2.283\n",
      "[0, 55000] loss: 2.242\n",
      "[0, 57500] loss: 2.117\n",
      "[0, 60000] loss: 1.652\n",
      "[0,  2500] loss: 1.242\n",
      "[0,  5000] loss: 1.200\n",
      "[0,  7500] loss: 0.927\n",
      "[0, 10000] loss: 0.831\n",
      "[0, 12500] loss: 0.869\n",
      "[0, 15000] loss: 0.761\n",
      "[0, 17500] loss: 0.730\n",
      "[0, 20000] loss: 0.648\n",
      "[0, 22500] loss: 0.625\n",
      "[0, 25000] loss: 0.594\n",
      "[0, 27500] loss: 0.611\n",
      "[0, 30000] loss: 0.518\n",
      "[0, 32500] loss: 0.532\n",
      "[0, 35000] loss: 0.513\n",
      "[0, 37500] loss: 0.435\n",
      "[0, 40000] loss: 0.516\n",
      "[0, 42500] loss: 0.488\n",
      "[0, 45000] loss: 0.564\n",
      "[0, 47500] loss: 0.539\n",
      "[0, 50000] loss: 0.448\n",
      "[0, 52500] loss: 0.561\n",
      "[0, 55000] loss: 0.515\n",
      "[0, 57500] loss: 0.481\n",
      "[0, 60000] loss: 0.463\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "\n",
    "    # full_loss_epoch[epoch] = calculate_loss_grad(net, trainloader, criterion, random_iter, n_samples)\n",
    "    # epoch += 1\n",
    "    # get the inputs\n",
    "\n",
    "    inputs, labels = data\n",
    "    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "    net.zero_grad()\n",
    "    i_data, loss = partial_grad(net, criterion, inputs, labels, i, random_iter)\n",
    "    optm.set_step_information({'current_datapoint': i_data})\n",
    "    optm.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    if i % 2500 == 2499:  # print every 2500 mini-batches\n",
    "        print('[%d, %5d] loss: %.3f' %\n",
    "              (epoch, (i) % n_samples + 1, running_loss / 2500))\n",
    "        running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "czOTg5rXf6Pv",
    "outputId": "ff34f3a8-392c-4961-eade-11ac8c60779b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 60000 test images: 89.705\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for i, data in enumerate(trainset):\n",
    "  inputs, labels = data\n",
    "  inputs = Variable(inputs.unsqueeze(0))\n",
    "  outputs = net(Variable(inputs))\n",
    "  predicted = torch.max(outputs.data, 1)[1]\n",
    "  total += 1\n",
    "  if predicted == labels:\n",
    "    correct += 1\n",
    "  \n",
    "\n",
    "print('Accuracy of the network on the 60000 test images:', 100 * (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lAADRE6PXrmG",
    "outputId": "587ceafd-8e46-4190-a59f-29f60e4e9223"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 89.86\n"
     ]
    }
   ],
   "source": [
    "testset = torchvision.datasets.MNIST(root='.', train=False,\n",
    "\t\t\t\t\t\t\t\t\t\tdownload=True, transform=transform)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for i, data in enumerate(testset):\n",
    "  inputs, labels = data\n",
    "  inputs = Variable(inputs.unsqueeze(0))\n",
    "  outputs = net(Variable(inputs))\n",
    "  predicted = torch.max(outputs.data, 1)[1]\n",
    "  total += 1\n",
    "  if predicted == labels:\n",
    "    correct += 1\n",
    "  \n",
    "\n",
    "print('Accuracy of the network on the 10000 test images:', 100 * (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Yoyw_iROigZJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sag_mnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
